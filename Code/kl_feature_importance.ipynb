{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import Utils.dataframe as dataframe_helper\n",
    "import Utils.learning as learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_one_hot(df:pd.DataFrame, N:int = None):\n",
    "    with open(\"../Data/top_word/top_y_word.txt\", \"r\") as f:\n",
    "        fake_words = dataframe_helper.string_to_list(f.read())\n",
    "  \n",
    "    with open(\"../Data/top_word/top_n_word.txt\", \"r\") as f:\n",
    "        non_fake_words = dataframe_helper.string_to_list(f.read())\n",
    "\n",
    "    fake_words = fake_words[:N]\n",
    "    non_fake_words = non_fake_words[:N]\n",
    "        \n",
    "    # KL one hot encoding\n",
    "    fakeWordOneHot = []\n",
    "    nonFakeWordOneHot = []\n",
    "\n",
    "    for content in df['reviewContent']:\n",
    "        fakeOneHot = ''\n",
    "        nonFakeOneHot = ''\n",
    "        words = re.findall(r'\\b[A-Za-z][a-z]{2,9}\\b', content)\n",
    "\n",
    "        for word in fake_words:\n",
    "            if word in words:\n",
    "                fakeOneHot += '1'\n",
    "            else:\n",
    "                fakeOneHot += '0'\n",
    "\n",
    "        for word in non_fake_words:\n",
    "            if word in words:\n",
    "                nonFakeOneHot += '1'\n",
    "            else:\n",
    "                nonFakeOneHot += '0'\n",
    "                \n",
    "        fakeWordOneHot.append(fakeOneHot)\n",
    "        nonFakeWordOneHot.append(nonFakeOneHot)\n",
    "    \n",
    "    df['fakeWordsOneHot'] = fakeWordOneHot\n",
    "    df['nonFakeWordsOneHot'] = nonFakeWordOneHot\n",
    "\n",
    "    df = dataframe_helper.onehot(df, 'fakeWordsOneHot', fake_words, 'fake')\n",
    "    df = dataframe_helper.onehot(df, 'nonFakeWordsOneHot', non_fake_words, 'non fake')\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_word_count(df:pd.DataFrame, N:int = None):\n",
    "    with open(\"../Data/top_word/top_y_word.txt\", \"r\") as f:\n",
    "        fake_words = dataframe_helper.string_to_list(f.read())\n",
    "  \n",
    "    with open(\"../Data/top_word/top_n_word.txt\", \"r\") as f:\n",
    "        non_fake_words = dataframe_helper.string_to_list(f.read())\n",
    "\n",
    "    fake_words = fake_words[:N]\n",
    "    non_fake_words = non_fake_words[:N]\n",
    "        \n",
    "    # KL word count\n",
    "    fakeWordsCount = []\n",
    "    nonFakeWordsCount = []\n",
    "\n",
    "    for content in df['reviewContent']:\n",
    "        words = re.findall(r'\\b[A-Za-z][a-z]{2,9}\\b', content)\n",
    "        fakeCount = 0\n",
    "        nonFakeCount = 0\n",
    "        for word in words:\n",
    "            if word in fake_words:\n",
    "                fakeCount += 1\n",
    "            elif word in non_fake_words:\n",
    "                nonFakeCount += 1\n",
    "        fakeWordsCount.append(fakeCount)\n",
    "        nonFakeWordsCount.append(nonFakeCount)\n",
    "    \n",
    "    df['fakeWordsCount'] = fakeWordsCount\n",
    "    df['nonFakeWordsCount'] = nonFakeWordsCount\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataframe_helper.load_data()\n",
    "df = dataframe_helper.data_cleaning(df)\n",
    "df = feature_engineering_one_hot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "under_sampled_df = dataframe_helper.under_sampling(df=df, target='flagged', big_sample='Y', small_sample='N')\n",
    "under_sampled_df.reset_index(drop=True, inplace=True)\n",
    "# under_sampled_df.to_csv('../Data/under_sampled_KL_df.csv', index=False)\n",
    "# under_sampled_df = pd.read_csv('../Data/under_sampled_KL_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_sampled_df = dataframe_helper.over_sampling(df=df, target='flagged', big_sample='Y', small_sample='N')\n",
    "over_sampled_df.reset_index(drop=True, inplace=True)\n",
    "# over_sampled_df.to_csv('../Data/over_sampled_KL_df.csv', index=False)\n",
    "# over_sampled_df = pd.read_csv('../Data/over_sampled_KL_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=42, criterion='entropy', max_depth=14, max_features='auto',\n",
    "                            n_estimators=500)\n",
    "nb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, results, feature = learning.semi_supervised_learning(df, model=rf, threshold=0.7, iterations=15, algorithm='Random Forest')\n",
    "# model, results, feature = learning.semi_supervised_learning(df, model=nb, threshold=0.7, iterations=15, algorithm='Naive Bayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, results, feature = learning.semi_supervised_learning(under_sampled_df, model=rf, threshold=0.7, iterations=15, algorithm='Random Forest')\n",
    "# model, results, feature = learning.semi_supervised_learning(under_sampled_df, model=nb, threshold=0.7, iterations=15, algorithm='Naive Bayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, results, feature = learning.semi_supervised_learning(over_sampled_df, model=rf, threshold=0.7, iterations=15, algorithm='Random Forest')\n",
    "# model, results, feature = learning.semi_supervised_learning(over_sampled_df, model=nb, threshold=0.7, iterations=15, algorithm='Naive Bayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = pd.Series(model.feature_importances_, index=feature)\n",
    "feat_importances.nlargest(100).plot(kind='barh',figsize=(25,25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort = model.feature_importances_.argsort()\n",
    "# plt.barh(boston.feature_names[sort], model.feature_importances_[sort])\n",
    "# plt.xlabel(\"Feature Importance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (tags/v3.9.12:b28265d, Mar 23 2022, 23:52:46) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d2032d7104da15a9d10fd1a2c136158f111ee2ad9d0f9c669774920547ee53a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
