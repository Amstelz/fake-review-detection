{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, pairwise_distances\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from pythainlp.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(df):\n",
    "    # Removing \\n from date field\n",
    "    for i in range(len(df['date'])):\n",
    "        if df['date'][i][0] == '\\n':\n",
    "            df['date'][i] = df['date'][i][1:]\n",
    "\n",
    "    # Making yelpJoinDate Format Uniform\n",
    "    df['yelpJoinDate'] = df['yelpJoinDate'].apply(\n",
    "        lambda x: datetime.strftime(datetime.strptime(x, '%B %Y'), '01/%m/%Y'))\n",
    "\n",
    "    # Pre-processing Text Reviews\n",
    "    # Remove Symbols\n",
    "    df['reviewContent'] = df['reviewContent'].apply(\n",
    "        lambda x: ''.join(char for char in str(x) if char not in '!\"#$&\\'()*.:;<=>?@[\\\\]^_`{|}~'))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    mnr_df1 = df[['reviewerID', 'date']].copy()\n",
    "    mnr_df2 = mnr_df1.groupby(by=['date', 'reviewerID']).size().reset_index(name='reviewPerDay')\n",
    "    mnr_df2['scaledReviewPerDay'] = mnr_df2['reviewPerDay'] / mnr_df2['reviewPerDay'].max()\n",
    "    mnr_df2.drop(columns=['reviewPerDay'], inplace=True)\n",
    "    df = df.merge(mnr_df2, on=['reviewerID', 'date'], how='inner')\n",
    "\n",
    "    # Review Length\n",
    "    df['reviewsLength'] = df['reviewContent'].apply(\n",
    "        lambda x: len(word_tokenize(x, engine=\"newmm\")))\n",
    "\n",
    "    # Review Deviation\n",
    "    df['reviewsDeviation'] = abs(df['rating'] - df['restaurantRating']) / 4\n",
    "\n",
    "    # Maximum cosine similarity\n",
    "    review_data = df\n",
    "\n",
    "    res = OrderedDict()\n",
    "\n",
    "    # Iterate over data and create groups of reviewers\n",
    "    for row in review_data.iterrows():\n",
    "        if row[1].reviewerID in res:\n",
    "            res[row[1].reviewerID].append(row[1].reviewContent)\n",
    "        else:\n",
    "            res[row[1].reviewerID] = [row[1].reviewContent]\n",
    "\n",
    "    individual_reviewer = [{'reviewerID': k, 'reviewContent': v} for k, v in res.items()]\n",
    "    df2 = dict()\n",
    "    df2['reviewerID'] = pd.Series([])\n",
    "    df2['maximumContentSimilarity'] = pd.Series([])\n",
    "    vector = TfidfVectorizer(min_df=0)\n",
    "    count = -1\n",
    "    for reviewer_data in individual_reviewer:\n",
    "        count = count + 1\n",
    "        # Handle Null/single review gracefully -24-Apr-2019\n",
    "        try:\n",
    "            tfidf = vector.fit_transform(reviewer_data['reviewContent'])\n",
    "        except:\n",
    "            pass\n",
    "        cosine = 1 - pairwise_distances(tfidf, metric='cosine')\n",
    "\n",
    "        np.fill_diagonal(cosine, -np.inf)\n",
    "        max = cosine.max()\n",
    "\n",
    "        # To handle reviewier with just 1 review\n",
    "        if max == -np.inf:\n",
    "            max = 0\n",
    "        df2['reviewerID'][count] = reviewer_data['reviewerID']\n",
    "        df2['maximumContentSimilarity'][count] = max\n",
    "\n",
    "    df3 = pd.DataFrame(df2, columns=['reviewerID', 'maximumContentSimilarity'])\n",
    "\n",
    "    # left outer join on original datamatrix and cosine dataframe -24-Apr-2019\n",
    "    df = pd.merge(review_data, df3, on=\"reviewerID\", how=\"left\")\n",
    "\n",
    "    df.drop(index=np.where(pd.isnull(df))[0], axis=0, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def under_sampling(df):\n",
    "    print(\"Under-Sampling Data\")\n",
    "\n",
    "    sample_size = len(df[(df['flagged'] == 'Y')])\n",
    "\n",
    "    authentic_reviews_df = df[df['flagged'] == 'N']\n",
    "    fake_reviews_df = df[df['flagged'] == 'Y']\n",
    "\n",
    "    authentic_reviews_us_df = authentic_reviews_df.sample(sample_size)\n",
    "    under_sampled_df = pd.concat([authentic_reviews_us_df, fake_reviews_df], axis=0)\n",
    "\n",
    "    print(\"Under-Sampling Complete\")\n",
    "    return under_sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def over_sampling(df):\n",
    "    print(\"Over-Sampling Data\")\n",
    "\n",
    "    sample_size = len(df[(df['flagged'] == 'N')])\n",
    "\n",
    "    authentic_reviews_df = df[df['flagged'] == 'N']\n",
    "    fake_reviews_df = df[df['flagged'] == 'Y']\n",
    "\n",
    "    fake_reviews_os_df = fake_reviews_df.sample(sample_size, replace=True)\n",
    "    over_sampled_df = pd.concat([authentic_reviews_df, fake_reviews_os_df], axis=0)\n",
    "\n",
    "    print(\"Over-Sampling Complete\")\n",
    "    return over_sampled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes, title=None, cmap=plt.cm.Blues):\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=classes,\n",
    "           yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervised_learning(df, model, algorithm):\n",
    "    df = df.copy()\n",
    "    print(\"Training \"+algorithm+\" Model\")\n",
    "    labels = df['flagged']\n",
    "\n",
    "    df.drop(['reviewID', 'reviewerID', 'restaurantID', 'date', 'name', 'location', 'yelpJoinDate', 'flagged',\n",
    "             'reviewContent', 'restaurantRating'], axis=1, inplace=True)\n",
    "\n",
    "    train_data, test_data, train_label, test_label = train_test_split(df, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "    # param_grid = {\n",
    "    #     'n_estimators': [10, 500],\n",
    "    #     'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    #     'max_depth': [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    "    #     'criterion': ['gini', 'entropy']\n",
    "    # }\n",
    "    # grid_clf_acc = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "    #\n",
    "    # grid_clf_acc.fit(train_data, train_label)\n",
    "\n",
    "    model.fit(train_data, train_label)\n",
    "    predicted_labels = model.predict(test_data)\n",
    "\n",
    "    # print('Best Params : ', grid_clf_acc.best_params_)\n",
    "    print(algorithm + ' Model Results')\n",
    "    print('--' * 20)\n",
    "    print('Accuracy Score : ' + str(accuracy_score(test_label, predicted_labels)))\n",
    "    print('Precision Score : ' + str(precision_score(test_label, predicted_labels, pos_label=\"Y\")))\n",
    "    print('Recall Score : ' + str(recall_score(test_label, predicted_labels, pos_label=\"Y\")))\n",
    "    print('F1 Score : ' + str(f1_score(test_label, predicted_labels, pos_label=\"Y\")))\n",
    "    print('Confusion Matrix : \\n' + str(confusion_matrix(test_label, predicted_labels)))\n",
    "    plot_confusion_matrix(test_label, predicted_labels, classes=['N', 'Y'],\n",
    "                          title=algorithm + ' Confusion Matrix').show()\n",
    "\n",
    "    return test_label, predicted_labels, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semi_supervised_learning(df, model, algorithm, threshold=0.8, iterations=40):\n",
    "    df = df.copy()\n",
    "    print(\"Training \"+algorithm+\" Model\")\n",
    "    labels = df['flagged']\n",
    "\n",
    "    df.drop(['reviewID', 'reviewerID', 'restaurantID', 'date', 'name', 'location', 'yelpJoinDate', 'flagged', 'restaurantRating'], axis=1, inplace=True)\n",
    "    # df.drop(['reviewID', 'reviewerID', 'restaurantID', 'date', 'name', 'location', 'yelpJoinDate',\n",
    "    #          'flagged', 'restaurantRating', 'usefulCount', 'coolCount', 'funnyCount',\n",
    "    #          'complimentCount', 'tipCount', 'fanCount'], axis=1, inplace=True)\n",
    "\n",
    "    train_data, test_data, train_label, test_label = train_test_split(df, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "    test_review_content = test_data['reviewContent']\n",
    "    train_data.drop(['reviewContent'], axis=1, inplace=True)\n",
    "    test_data.drop(['reviewContent'], axis=1, inplace=True)\n",
    "    \n",
    "    test_data_copy = test_data.copy()\n",
    "    test_label_copy = test_label.copy()\n",
    "\n",
    "    all_labeled = False\n",
    "\n",
    "    current_iteration = 0\n",
    "\n",
    "    # param_grid = {\n",
    "    #     'n_estimators': [10, 500],\n",
    "    #     'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    #     'max_depth': [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    "    #     'criterion': ['gini', 'entropy']\n",
    "    # }\n",
    "    # grid_clf_acc = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "    #\n",
    "    # grid_clf_acc.fit(train_data, train_label)\n",
    "\n",
    "    pbar = tqdm(total=iterations)\n",
    "\n",
    "    while not all_labeled and (current_iteration < iterations):\n",
    "        # print(\"Before train data length : \", len(train_data))\n",
    "        # print(\"Before test data length : \", len(test_data))\n",
    "        current_iteration += 1\n",
    "        model.fit(train_data, train_label)\n",
    "\n",
    "        probabilities = model.predict_proba(test_data)\n",
    "        pseudo_labels = model.predict(test_data)\n",
    "\n",
    "        indices = np.argwhere(probabilities > threshold)\n",
    "\n",
    "        # print(\"rows above threshold : \", len(indices))\n",
    "        for item in indices:\n",
    "            train_data.loc[test_data.index[item[0]]] = test_data.iloc[item[0]]\n",
    "            train_label.loc[test_data.index[item[0]]] = pseudo_labels[item[0]]\n",
    "        test_data.drop(test_data.index[indices[:, 0]], inplace=True)\n",
    "        test_label.drop(test_label.index[indices[:, 0]], inplace=True)\n",
    "        # print(\"After train data length : \", len(train_data))\n",
    "        # print(\"After test data length : \", len(test_data))\n",
    "        print(\"--\" * 20)\n",
    "\n",
    "        if len(test_data) == 0:\n",
    "            print(\"Exiting loop\")\n",
    "            all_labeled = True\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    predicted_labels = model.predict(test_data_copy)\n",
    "\n",
    "    # print('Best Params : ', grid_clf_acc.best_params_)\n",
    "    print(algorithm + ' Model Results')\n",
    "    print('--' * 20)\n",
    "    print('Accuracy Score : ' + str(accuracy_score(test_label_copy, predicted_labels)))\n",
    "    print('Precision Score : ' + str(precision_score(test_label_copy, predicted_labels, pos_label=\"Y\")))\n",
    "    print('Recall Score : ' + str(recall_score(test_label_copy, predicted_labels, pos_label=\"Y\")))\n",
    "    print('F1 Score : ' + str(f1_score(test_label_copy, predicted_labels, pos_label=\"Y\")))\n",
    "    print('Confusion Matrix : \\n' + str(confusion_matrix(test_label_copy, predicted_labels)))\n",
    "    plot_confusion_matrix(test_label_copy, predicted_labels, classes=['N', 'Y'],\n",
    "                          title=algorithm + ' Confusion Matrix').show()\n",
    "\n",
    "    results = test_data_copy.copy()\n",
    "    results['reviewContent'] = test_review_content\n",
    "    results['flagged'] = test_label_copy\n",
    "    results['predicted'] = predicted_labels\n",
    "\n",
    "    return model, results\n",
    "                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../Data/raw_thai_df.csv', index_col=0)\n",
    "# df = data_cleaning(df)\n",
    "# df = feature_engineering(df)\n",
    "# df.to_csv('../Data/thai_df.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['reviewID', 'reviewerID', 'restaurantID', 'date', 'rating',\n",
       "       'reviewUsefulCount', 'reviewContent', 'flagged', 'name', 'location',\n",
       "       'yelpJoinDate', 'friendCount', 'reviewCount', 'firstCount',\n",
       "       'usefulCount', 'coolCount', 'funnyCount', 'complimentCount', 'tipCount',\n",
       "       'fanCount', 'restaurantRating', 'scaledReviewPerDay', 'reviewsLength',\n",
       "       'reviewsDeviation', 'maximumContentSimilarity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../Data/thai_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# under_sampled_df = under_sampling(df)\n",
    "# under_sampled_df.to_csv('../Data/thai_under_sampled_df.csv', index=False)\n",
    "under_sampled_df = pd.read_csv('../Data/thai_under_sampled_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# over_sampled_df = over_sampling(df)\n",
    "# over_sampled_df.to_csv('../Data/thai_over_sampled_df.csv', index=False)\n",
    "over_sampled_df = pd.read_csv('../Data/thai_over_sampled_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.drop(['reviewID', 'reviewerID', 'restaurantID', 'date', 'name', 'location', 'yelpJoinDate', 'flagged', 'reviewContent', 'restaurantRating', 'usefulCount', 'coolCount', 'funnyCount',\n",
    "                     'complimentCount', 'tipCount', 'fanCount'], axis=1)\n",
    "train_df.to_csv('../Data/thai_train_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train DF Columns\n",
    "- rating = comment rating **<span style=\"color:CornflowerBlue\">(raw review file)</span>**\n",
    "- reviewUsefulCount = number of user's review useful count raw **<span style=\"color:CornflowerBlue\">(count form raw review file)</span>** \n",
    "- friendCount = number of user's friend raw **<span style=\"color:CornflowerBlue\">(join form user file)</span>** \n",
    "- reviewCount = number of user's review count raw **<span style=\"color:CornflowerBlue\">(join form user file)</span>** \n",
    "- firstCount = number of user's first comment raw **<span style=\"color:CornflowerBlue\">(count form raw review file)</span>** \n",
    "- usefulCount = number of comment's useful count raw **<span style=\"color:CornflowerBlue\">(raw review file)</span>** \n",
    "- coolCount = number of comment's cool count raw **<span style=\"color:CornflowerBlue\">(raw review file)</span>** \n",
    "- funnyCount = number of comment's funny count raw **<span style=\"color:CornflowerBlue\">(raw review file)</span>** \n",
    "- complimentCount = review's compliment count raw **<span style=\"color:CornflowerBlue\">(join form tip file)</span>** \n",
    "- tipCount = user's tip count raw **<span style=\"color:CornflowerBlue\">(join form tip file)</span>** \n",
    "- fanCount = number of user's fan raw **<span style=\"color:CornflowerBlue\">(join form user file)</span>** \n",
    "- scaledReviewPerDay = scaled review per day raw **<span style=\"color:CornflowerBlue\">(calculate from raw review file)</span>** \n",
    "- reviewsLength = review length raw **<span style=\"color:CornflowerBlue\">(calculate from raw review file)</span>** \n",
    "- reviewsDeviation = store rating - user rating raw **<span style=\"color:CornflowerBlue\">(calculate form businees and reviews file)</span>**\n",
    "- maximumContentSimilarity = maximun content similarity raw **<span style=\"color:CornflowerBlue\">(calculate from raw review file)</span>** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# from autoviz.AutoViz_Class import AutoViz_Class\n",
    "# autoviz = AutoViz_Class().AutoViz('../Data/thai_train_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=42, criterion='entropy', max_depth=14, max_features='auto',\n",
    "                            n_estimators=500)\n",
    "nb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_supervised_learning(df, model=rf, threshold=0.7, iterations=15, algorithm='Random Forest')\n",
    "semi_supervised_learning(df, model=nb, threshold=0.7, iterations=15, algorithm='Naive Bayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_supervised_learning(under_sampled_df, model=rf, threshold=0.7, iterations=15, algorithm='Random Forest')\n",
    "semi_supervised_learning(under_sampled_df, model=nb, threshold=0.7, iterations=15, algorithm='Naive Bayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, results = semi_supervised_learning(over_sampled_df, model=rf, threshold=0.7, iterations=15, algorithm='Random Forest')\n",
    "semi_supervised_learning(over_sampled_df, model=nb, threshold=0.7, iterations=15, algorithm='Naive Bayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('../Data/thai_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d2032d7104da15a9d10fd1a2c136158f111ee2ad9d0f9c669774920547ee53a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
