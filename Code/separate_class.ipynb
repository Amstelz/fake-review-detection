{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "import Utils.plot as plot_helper\n",
    "import Utils.dataframe as dataframe_helper\n",
    "import Utils.dict as dict_helper\n",
    "import Utils.calculate as calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_WORDS = 1000\n",
    "N = 10\n",
    "NUMBER_OF_SELECTED_WORDS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataframe_helper.load_data()\n",
    "df = dataframe_helper.data_cleaning(df)\n",
    "df = df[['reviewContent', 'flagged']]\n",
    "df['flagged'].replace({'Y': 'Fake', 'N':'Genuine'}, inplace=True)\n",
    "\n",
    "fake_df = df[df['flagged'] == 'Fake'].reset_index(drop=True)\n",
    "genuine_df = df[df['flagged'] == 'Genuine'].reset_index(drop=True)\n",
    "all_df = pd.concat([fake_df, genuine_df]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../Data/BBC/BBC_News_Train.csv')\n",
    "\n",
    "# sport_df = df[df['Category'] == 'sport']\n",
    "# sport_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# tech_df = df[df['Category'] == 'tech']\n",
    "# tech_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# all_df = pd.concat([sport_df, tech_df])\n",
    "# all_df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_string_fake = dataframe_helper.get_str_from_df(fake_df, 10, 'reviewContent')\n",
    "text_string_genuine = dataframe_helper.get_str_from_df(genuine_df, 10, 'reviewContent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open ('../Data/book/fake_book.txt', 'r', encoding='utf-8' ) as f:\n",
    "#      text_string_fake = f.read().lower()\n",
    "\n",
    "# with open ('../Data/book/genuine_book.txt', 'r', encoding='utf-8' ) as f:\n",
    "#      text_string_genuine = f.read().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# zipf distribution\n",
    "- from zipf's law most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word, etc.\n",
    "\n",
    "    ![Zipf_distribution_CMF](../Picture/Zipf_distribution_CMF.png \"Zipf_distribution_CMF\") \n",
    "    \n",
    "    Zipf_distribution_CMF\n",
    "\n",
    "    ![Zipf_distribution_PMF](../Picture/Zipf_distribution_PMF.png \"Zipf_distribution_PMF\") \n",
    "    \n",
    "    Zipf_distribution_PMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_fake = dict_helper.get_most_frequent(text_string_fake, remove_stop_word=False)\n",
    "dict_helper.print_most_frequent(dict_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_genuine = dict_helper.get_most_frequent(text_string_genuine, remove_stop_word=False)\n",
    "dict_helper.print_most_frequent(dict_genuine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dict = {'fake copus':dict_fake, 'genuine copus':dict_genuine}\n",
    "plot_helper.zipf_plot(combined_dict, 50, 20, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with stop words\n",
    "common_keys = dict_fake.keys() & dict_genuine.keys()\n",
    "common_dict_fake = {key: dict_fake[key] for key in common_keys}\n",
    "common_dict_fake = dict(sorted(common_dict_fake.items(), key=lambda elem: elem[1], reverse=True))\n",
    "common_dict_genuine = {key: dict_genuine[key] for key in common_dict_fake.keys()}\n",
    "\n",
    "# with stop words\n",
    "sum_fake = sum(list(common_dict_fake.values()))\n",
    "sum_genuine = sum(list(common_dict_genuine.values()))\n",
    "\n",
    "# with stop words\n",
    "words_list = list(common_dict_fake.keys())[:NUMBER_OF_SELECTED_WORDS]\n",
    "prob_common_dict_fake = [x/sum_fake for x in list(common_dict_fake.values())[:NUMBER_OF_SELECTED_WORDS]]\n",
    "prob_common_dict_genuine = [x/sum_genuine for x in list(common_dict_genuine.values())[:NUMBER_OF_SELECTED_WORDS]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the kl divergence\n",
    "kl_pq = calculate.kl_divergence(prob_common_dict_fake, prob_common_dict_genuine)\n",
    "print('KL(P || Q): %.3f bits' % kl_pq)\n",
    "kl_qp = calculate.kl_divergence(prob_common_dict_genuine, prob_common_dict_fake)\n",
    "print('KL(Q || P): %.3f bits' % kl_qp)\n",
    "\n",
    "delta_words = calculate.delta_kl_divergence_list(prob_common_dict_fake, prob_common_dict_genuine)\n",
    "\n",
    "positive_deltas = [x for x in delta_words if x >= 0]\n",
    "positive_indexs = [i for i,x in enumerate(delta_words) if x >= 0]\n",
    "positive_words = [words_list[i] for i in positive_indexs]\n",
    "\n",
    "negative_deltas = [x for x in delta_words if x < 0]\n",
    "negative_indexs = [i for i,x in enumerate(delta_words) if x < 0]\n",
    "negative_words = [words_list[i] for i in negative_indexs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_helper.print_word_and_delta(positive_words, positive_deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_helper.print_comporison(positive_words, common_dict_fake, common_dict_genuine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_helper.print_word_and_delta(negative_words, negative_deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_helper.print_comporison(negative_words, common_dict_fake, common_dict_genuine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_words_dict = {words_list[i]: delta_words[i] for i in range(len(delta_words))}\n",
    "delta_words_dict = dict(sorted(delta_words_dict.items(), key=lambda elem: elem[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_helper.plot_words_distribution(prob_common_dict_fake, prob_common_dict_genuine, words_list, 30, 15, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_delta_words_dict = {words_list[i]: abs(delta_words[i]) for i in range(len(delta_words))}\n",
    "abs_delta_words_dict = dict(sorted(abs_delta_words_dict.items(), key=lambda elem: elem[1], reverse=True))\n",
    "\n",
    "top_words = list(abs_delta_words_dict.keys())[:TOP_WORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in top_words:\n",
    "    print(f'{i}: {delta_words_dict[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_fake = {i:delta_words_dict[i] for i in top_words if delta_words_dict[i] > 0}\n",
    "top_words_genuine = {i:delta_words_dict[i] for i in top_words if delta_words_dict[i] < 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_genuine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_top_words_fake = {i[0]:i[1] for i in top_words_fake.items() if i[0] not in stop}\n",
    "pure_top_words_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_top_words_genuine = {i[0]:i[1] for i in top_words_genuine.items()  if i[0] not in stop}\n",
    "pure_top_words_genuine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_top_genuine_words_fake = [i[0] for i in list(pure_top_words_fake.items())[:N]]\n",
    "pure_top_genuine_words_genuine = [i[0] for i in list(pure_top_words_genuine.items())[:N]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../Data/top_word/top{N}_fake_word.txt\", \"w\") as output:\n",
    "    output.write(str(pure_top_genuine_words_fake ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../Data/top_word/top{N}_genuine_word.txt\", \"w\") as output:\n",
    "    output.write(str(pure_top_genuine_words_genuine ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pure_top_words_fake.items()), len(pure_top_words_genuine.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../Data/top_word/top_fake_word.txt\", \"w\") as output:\n",
    "    output.write(str([i[0] for i in pure_top_words_fake.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../Data/top_word/top_genuine_word.txt\", \"w\") as output:\n",
    "    output.write(str([i[0] for i in pure_top_words_genuine.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_top_words_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_top_words_genuine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate.print_summary_kl('cup', common_dict_fake, common_dict_genuine)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF Approch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagOfWordsA = re.findall(r'\\b[A-Za-z][a-z]{2,9}\\b',  text_string_fake)\n",
    "bagOfWordsB = re.findall(r'\\b[A-Za-z][a-z]{2,9}\\b',  text_string_genuine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfWordsA = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsA:\n",
    "    numOfWordsA[word] += 1\n",
    "numOfWordsB = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsB:\n",
    "    numOfWordsB[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfA = calculate.computeTF(numOfWordsA, bagOfWordsA)\n",
    "tfB = calculate.computeTF(numOfWordsB, bagOfWordsB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tfA), len(tfB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fake_df), len(genuine_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idfs = calculate.computeIDF(uniqueWords, all_df, 'reviewContent')\n",
    "# idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Data/tfidf/idfs.json\", \"r\") as json_file:\n",
    "    idfs = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfA = calculate.computeTFIDF(tfA, idfs)\n",
    "tfidfB = calculate.computeTFIDF(tfB, idfs)\n",
    "df = pd.DataFrame([tfidfA, tfidfB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = ['fake', 'genuine']\n",
    "tfidf = df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.loc[(tfidf['fake'] != 0) | (tfidf['genuine'] != 0)].sort_values(by='fake', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.loc[(tfidf['fake'] != 0) | (tfidf['genuine'] != 0)].sort_values(by='genuine', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate.print_summary_tf_idf('married', tfA, tfB, idfs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate.print_summary('extra', common_dict_fake, common_dict_genuine, tfA, tfB, idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.sort_values(by='fake', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.sort_values(by='genuine', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.loc[(tfidf['fake'] != 0) & (tfidf['genuine'] != 0)].sort_values(by='fake', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf.to_csv('../Data/tfidf/tfidf.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../Data/tfidf/idfs.json\", \"w\") as outfile:\n",
    "#     json.dump(idfs, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d2032d7104da15a9d10fd1a2c136158f111ee2ad9d0f9c669774920547ee53a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
